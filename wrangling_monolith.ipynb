{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import os\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket_name = 'yt-charts-raw'\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# List all blobs in the bucket\n",
    "blobs = bucket.list_blobs()\n",
    "\n",
    "# Loop through each blob (file)\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith('.csv'):\n",
    "        # Download blob data into memory and create DataFrame\n",
    "        byte_stream = BytesIO(blob.download_as_bytes())\n",
    "        df = pd.read_csv(byte_stream)\n",
    "        \n",
    "        # Add DataFrame to list\n",
    "        df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into one DataFrame\n",
    "weekly_charts = pd.concat(df_list)\n",
    "weekly_charts.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Published dates and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=os.environ['YOUTUBE_API_KEY'])\n",
    "\n",
    "weekly_charts['track_video_id_youtube'] = weekly_charts['YouTube URL'].apply(lambda x: x.split(\"v=\")[1])\n",
    "\n",
    "\n",
    "published_dates = []\n",
    "tags_data = []\n",
    "\n",
    "for video_id in weekly_charts['track_video_id_youtube']:\n",
    "    video_request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    video_response = video_request.execute()\n",
    "    \n",
    "    if not video_response.get('items'):\n",
    "        print(f\"No data returned for video ID: {video_id}\")\n",
    "        published_dates.append(None)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        published_date = video_response['items'][0]['snippet']['publishedAt']\n",
    "        published_dates.append(published_date)\n",
    "    except KeyError:\n",
    "        print(f\"Could not retrieve published date for video ID: {video_id}\")\n",
    "        published_dates.append(None)\n",
    "\n",
    "    try:\n",
    "        tags = video_response['items'][0]['snippet'].get('tags', [])[:3]\n",
    "        for tag in tags:\n",
    "            tags_data.append({'track_video_id_youtube': video_id, 'tags': tag})\n",
    "    except KeyError:\n",
    "        print(f\"Could not retrieve tags for video ID: {video_id}\")\n",
    "\n",
    "tags_df = pd.DataFrame(tags_data)\n",
    "\n",
    "weekly_charts['yt_published_dates'] = published_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Api calls and creating dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Spotipy\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=os.getenv(\"SPOTIPY_CLIENT_ID\"),\n",
    "                                                           client_secret=os.getenv(\"SPOTIPY_CLIENT_SECRET\")))\n",
    "\n",
    "# Generate ID function\n",
    "def generate_id(value):\n",
    "    return hashlib.md5(str(value).encode()).hexdigest()\n",
    "\n",
    "def get_spotify_id(track):\n",
    "    try:\n",
    "        if len(track) > 100:  # Truncate query to first 100 characters\n",
    "            track = track[:100]\n",
    "        result = sp.search(q=track, type='track', limit=1)\n",
    "        return result['tracks']['items'][0]['id'] if result['tracks']['items'] else None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get Spotify artist IDs with error handling\n",
    "def get_artist_id(artist):\n",
    "    try:\n",
    "        result = sp.search(q=artist, type='artist', limit=1)\n",
    "        return result['artists']['items'][0]['id'] if result['artists']['items'] else None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get genres with error handling\n",
    "def get_genre(spotify_id):\n",
    "    try:\n",
    "        if spotify_id is None:\n",
    "            return None\n",
    "        result = sp.artist(spotify_id)\n",
    "        return result['genres'][0] if result['genres'] else None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read your dataframe (already in your code as weekly_charts)\n",
    "\n",
    "# Prepare fact_chart\n",
    "fact_chart = weekly_charts[['Rank', 'Previous Rank', 'week_open', 'week_close', 'Weeks on Chart', 'Views', 'Weekly Growth']].copy()\n",
    "fact_chart['fact_id'] = fact_chart.apply(lambda x: generate_id(tuple(x)), axis=1)\n",
    "fact_chart['dim_track_id'] = weekly_charts.apply(lambda x: generate_id((x['Track Name'], x['track_video_id_youtube'])), axis=1)\n",
    "fact_chart['dim_artist_id'] = weekly_charts.apply(lambda x: generate_id((x['Artist Names'], x['main_artist'])), axis=1)\n",
    "\n",
    "\n",
    "# Prepare dim_track\n",
    "dim_track = weekly_charts[['Track Name', 'track_video_id_youtube']].drop_duplicates().copy()\n",
    "dim_track['dim_track_id'] = dim_track.apply(lambda x: generate_id(tuple(x)), axis=1)\n",
    "\n",
    "# Get Spotify track IDs\n",
    "dim_track['spotify_track_id'] = dim_track['Track Name'].apply(lambda x: get_spotify_id(x) if pd.notna(x) else None)\n",
    "\n",
    "# Prepare dim_artist\n",
    "dim_artist = weekly_charts[['Artist Names', 'main_artist']].drop_duplicates().copy()\n",
    "dim_artist['dim_artist_id'] = dim_artist.apply(lambda x: generate_id(tuple(x)), axis=1)\n",
    "\n",
    "# Get Spotify artist IDs\n",
    "dim_artist['artist_id_spotify'] = dim_artist['main_artist'].apply(lambda x: get_artist_id(x) if pd.notna(x) else None)\n",
    "\n",
    "# Prepare dim_genre\n",
    "dim_genre = pd.DataFrame()\n",
    "dim_genre['dim_genre_id'] = dim_artist['dim_artist_id'].apply(generate_id)\n",
    "\n",
    "# Get genres\n",
    "dim_genre['genre_name'] = dim_artist['artist_id_spotify'].apply(lambda x: get_genre(x) if x is not None else None)\n",
    "dim_genre['dim_artist_id'] = dim_artist['dim_artist_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifications and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [dim_genre, dim_artist, dim_track, fact_chart]\n",
    "\n",
    "for i in dfs:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests to check consistency\n",
    "\n",
    "# Check if dim_track_id in fact_chart aligns with dim_track\n",
    "assert all(fact_chart['dim_track_id'].isin(dim_track['dim_track_id']))\n",
    "\n",
    "# Check if dim_artist_id in fact_chart aligns with dim_artist\n",
    "assert all(fact_chart['dim_artist_id'].isin(dim_artist['dim_artist_id']))\n",
    "\n",
    "# Check if dim_genre_id in dim_genre aligns with dim_artist\n",
    "assert all(dim_genre['dim_artist_id'].isin(dim_artist['dim_artist_id']))\n",
    "\n",
    "# Check if all fact_ids are unique\n",
    "assert fact_chart['fact_id'].is_unique\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('yt-charts-trusted')\n",
    "\n",
    "for file_name in os.listdir(\"data/trusted\"):\n",
    "    if file_name.endswith('.csv'):\n",
    "        blob = bucket.blob(file_name)\n",
    "        blob.upload_from_filename(os.path.join(\"data/trusted\", file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket_name = 'yt-charts-trusted'\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "blobs = bucket.list_blobs()\n",
    "\n",
    "print(f\"Files in {bucket_name}:\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "for blob in blobs:\n",
    "    print(blob.name)\n",
    "    count +=1\n",
    "\n",
    "print(f\"{count} files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
